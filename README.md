# Vanguard-ab-test ğŸ† ğŸ“Š ğŸ“‘  ğŸ“‰ ğŸ“ˆ

### The Team:

## ğŸ‘“Loredane, Johana ğŸ““

## Projet Overview
The Vanguard project involved an A/B test conducted by the Customer Experience (CX) team to evaluate the impact of a new user interface (UI) and in-context prompts on user engagement and process completion rates. 
Project Context: As a newly employed data analyst, the task was to analyze the results of this digital experiment aimed at enhancing the online experience for Vanguard's clients. 

### Digital Challenge: 
With evolving client expectations, Vanguard hypothesized that a more intuitive and modern UI could streamline the online process and encourage higher completion rates for critical tasks.

### Experiment Details:
Duration: The A/B test ran from March 15, 2017, to June 20, 2017.
#### Groups:
##### Control Group: 
Interacted with the traditional online process.
##### Test Group: 
Engaged with the new, enhanced interface.

### Key Findings: ğŸ”
Initial exploration indicated that the number of logins was low, averaging only 1 to 2 times over six months. This raised concerns about the adequacy of the three-month testing period to draw definitive conclusions regarding the effectiveness of the new UI.

### Collaboration Tools: ğŸ§°
Over the two weeks of project work, the team maintained robust communication and organization. We utilized Trello for project management, Tableau for data visualization, Canvas for design discussions, and Google Docs for collaborative note-taking, where we compiled our ideas, hypotheses, and insights.

https://trello.com/b/bVYQU1Gf/w56-vaguardabtest 
<img width="565" alt="image" src="https://github.com/user-attachments/assets/8760f2a6-a1b0-4960-bea8-7980fa0eaa85">

## EDA and Data Cleaning: â„¹ï¸
In the exploratory data analysis (EDA) phase, we examined the dataset to understand its structure, quality, and key characteristics. This involved summarizing the data, checking for missing values, and identifying any anomalies or outliers. We conducted data cleaning to address these issues, which included removing duplicates, correcting inconsistent entries, and ensuring that categorical variables were properly formatted. This step was crucial to ensure the integrity of the dataset and prepare it for further analysis.

## Performance Metrics: ğŸ’¹
To evaluate the effectiveness of the new user interface, we defined key performance metrics focused on user engagement and task completion. The primary metrics included the overall completion rate of the online process, the average time spent on each step, and user login frequency. These metrics allow us to quantitatively assess the impact of the proposed changes and compare the performance of the control and test groups.

## Hypothesis Testing: ğŸ†
We formulated specific hypotheses to guide the analysis, primarily focusing on whether the new UI and in-context prompts would lead to higher completion rates compared to the traditional interface. Statistical hypothesis testing methods, such as t-tests or chi-square tests, were employed to compare the performance metrics between the control and test groups, enabling us to determine if observed differences were statistically significant.

## Experiment Evaluation: ğŸ“‘
In the experiment evaluation phase, we analyzed the results of the A/B test to understand the impact of the new interface on user behavior. This involved reviewing performance metrics to identify trends and patterns, assessing conversion rates, and understanding user engagement levels. The evaluation also took into account potential confounding factors that could influence the results, ensuring a comprehensive analysis of the experiment's outcomes.

## Visualization: ğŸ‘ï¸â€ğŸ—¨ï¸
We utilized Tableau for data visualization, to create clear and informative visual representations of the data and analysis results. Visualizations included histograms depicting age distributions, bar charts comparing completion rates between the control and test groups, and time series plots showing login frequencies. These visual insights not only aided in understanding the data but also communicated our findings effectively.
https://public.tableau.com/app/profile/loredane.nery.da.silva/viz/group_project_vanguard/Story1?publish=yes

# Conclusion: â˜‘ï¸
The Vanguard project aimed to evaluate the effectiveness of a redesigned user interface (UI) through an A/B test comparing two groups: a Control group using the existing UI and a Test group utilizing the new design enhanced with in-context prompts. 

The analysis revealed several key insights:

### Completion Rates: 
Statistical testing provided sufficient evidence to reject the null hypothesis suggesting that the completion rates for the Control and Test groups are equal. The Test group demonstrated higher completion rates, indicating potential improvements in user engagement due to the new design.
### Cost-Effectiveness Threshold: 
However, upon further analysis, we found that the completion rate for the Test group does not exceed the Control group's rate by the required 5% margin set by Vanguard for justifying the investment in the redesign. The p-value of 1.0 and the z-statistic of -31.8451 led us to fail to reject the null hypothesis regarding cost-effectiveness, highlighting the need for caution in implementing the new design based solely on these results.
### Demographic Insights: 
Additional hypotheses regarding the average age of clients engaging with the new process will provide further insights that can help Vanguard better understand user demographics and refine their approach to meet client needs effectively.
Here the link of our presentation:
https://www.canva.com/design/DAGPziwo5LY/gwZubIEtt1TfMewKwwpeMw/edit?ui=eyJEIjp7IkEiOnsiQSI6IktBR1FlcUNic3lNIiwiQyI6eyJBPyI6IkEiLCJBIjoiS0FHUWVxQ2JzeU0ifX19fQ 


